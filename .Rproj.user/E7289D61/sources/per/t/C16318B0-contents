seedsdata = read.csv('/Users/elias/Downloads/seeds_dataset_class (1).csv', sep=",")
#randomize - we do not want to train an ordered dataset, 
# highly unlikely to get higher accuracy, no possibility of backtracking
seeds_rand=seedsdata[sample(209,209),]

#separate the class and values into separate variables
seedsclass = seeds_rand[,1] 
seedsvalues = seeds_rand[,-1]

#set up a training set - split data into a larger portion for training
seedsclassTrain = seedsclass[1:150]
seedsvaluesTrain = seedsvalues[1:150,]

#and testset - split data into a smaller portion for testset
seedsclassTest = seedsclass[150:209]
seedsvaluesTest = seedsvalues[150:209,]

#building descision tree MODEL
#install rpart library for building classification and regression trees
library(rpart)
#creating a fit model (model for decision tree)
fit <- rpart(seedsclassTrain~., method="class", data=seedsvaluesTrain) #model for decision tree

#Plot descision tree for seedsdata
plot(fit, uniform=TRUE, main="Decision Tree for Seedsdata") #COULD ADD MORE MARGIN TO MAKE IT MORE READIBLE
text(fit, use.n=TRUE, all=TRUE, cex=.8) 


#classifier test / checking accuracy OF DECISION TREE MODEL
treepred <-predict(fit, seedsvaluesTest, type = 'class') #USING PREDICT FUNCTION
n = length(seedsclassTest) #the number of test cases in the length of the testdata
ncorrect = sum(treepred==seedsclassTest) #the number of correctly predicted
accuracy=ncorrect/n #number of correctly predicted / length of test data
print(accuracy)

#viewing results as table - WHY WE USE THIS!!!!!!!!!!!!!!
table_mat = table(seedsclassTest, treepred)
print(table_mat)

#vewing effect of pruning/plotting pruned dexision trees with a pruning parameter here, cp, set to 0.1
#REMOVING UNWANTED BRANCHES TO AVOID OVERFITTING
pfit1<- prune(fit, cp=0.1)

#vewing effect of pruning/with a pruning parameter here, cp, set to 0.3
pfit2<- prune(fit, cp=0.3)

#vewing effect of pruning/with a pruning parameter here, cp, set to 0.6
pfit3<- prune(fit, cp=0.6)

#testing the classifier on the test set by calculating the predictions for each testcase in our test set
ppred1 <- predict(pfit1, seedsvaluesTest, type = "class")
ppred2 <- predict(pfit2, seedsvaluesTest, type = "class")
ppred3 <- predict(pfit3, seedsvaluesTest, type = "class")

l = length(seedsclassTest)

lcor1 = sum(ppred1 == seedsclassTest)
lcor2 = sum(ppred2 == seedsclassTest)
lcor3 = sum(ppred3 == seedsclassTest)

lacc1 = lcor1/l #with a pruning parameter here, cp, set to 0.1
lacc2 = lcor2/l #with a pruning parameter here, cp, set to 0.3
lacc3 = lcor3/l #with a pruning parameter here, cp, set to 0.6

#declaring variables of decision tree
accuracyDT <- c(lacc1,lacc2,lacc3)
cpVAL <-c(0.1, 0.3, 0.6)

#plotting pruned tree
plot(pfit, uniform = TRUE, main="Pruned deision tree CP=0.1)")
text(pfit, use.n = TRUE, all = TRUE, cex =0.5)

#Task 3: Plot 2 variables (class and area)
plot(seedsclass, seeds_rand$Area, main = "Comparison of area of three variants of wheat kernel (area against class)", ylab="area", xlab = "seeds class", col = ifelse(seedsclass == 1, "green", ifelse(seedsclass == 2, "red", "blue")))

#compares two different values to see the accuracy
#Task 4:Compare the accuracy of the different pruned trees to KNN with different values of k
library(class)
#generating predicated classes k=number of clusters
knnpred1 = knn(seedsvaluesTrain, seedsvaluesTest, seedsclassTrain, k=1)
knnpred2 = knn(seedsvaluesTrain, seedsvaluesTest, seedsclassTrain, k=3)
knnpred3 = knn(seedsvaluesTrain, seedsvaluesTest, seedsclassTrain, k=6)

k = length(seedsclassTest)
kcor1 = sum(knnpred1 == seedsclassTest)
kcor2 = sum(knnpred2 == seedsclassTest)
kcor3 = sum(knnpred3 == seedsclassTest)

kacc1 = kcor1 / k
kacc2 = kcor2 / k
kacc3 = kcor3 / k

print(kacc1)
print(kacc2)
print(kacc3)
print(lacc2)

#declared variables of KNN
knnAccRes <- c(kacc1, kacc2, kacc3)
noClusters<- c(1,3,6)

#comparing knn and dynamic trees plots
plot(noClusters,knnAccRes, ylab = "k-nearest accuracy", xlab = "number of k-values", main = "accuracy against k-values")
plot(cpVAL, accuracyDT, ylab = "decision tree accuracy", xlab = "CP values", main="accuracy of pruned data")


